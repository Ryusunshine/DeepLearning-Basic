{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Linear Layer\n",
    "\n",
    "$$\\begin{gathered}\n",
    "y=x\\cdot{W}+b, \\\\\n",
    "\\text{where }x\\in\\mathbb{R}^{N\\times{n}}\\text{, }y\\in\\mathbb{R}^{N\\times{m}}. \\\\\n",
    "\\\\\n",
    "\\text{Thus, }W\\in\\mathbb{R}^{n\\times{m}}\\text{ and }b\\in\\mathbb{R}^m.\n",
    "\\end{gathered}$$\n",
    "\n",
    "W = 가중치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 수식을 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.FloatTensor([[1, 2],\n",
    "                       [3, 4],\n",
    "                       [5, 6]])\n",
    "\n",
    "# W = ([n,m]) = ([행,렬]) = ([3,2])\n",
    "b = torch.FloatTensor([2, 2])\n",
    "# b = ([m]) = (벡터?) = ([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(W.size())\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, W, b):\n",
    "    y = torch.matmul(x, W) + b\n",
    "    \n",
    "# x = ([N,n])\n",
    "# W = ([n,m])\n",
    "# x * W = ([N,n] * [n,m]) = ([N, m])\n",
    "#현재 n = 3, m = 2, N은 모름\n",
    "    \n",
    "    return y\n",
    "# y = ([N,m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 1, 1],\n",
    "                       [2, 2, 2],\n",
    "                       [3, 3, 3],\n",
    "                       [4, 4, 4]])\n",
    "#이제 X를 실제로 선언\n",
    "\n",
    "print(x.size())\n",
    "#대문자 N은 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = linear(x, W, b) #이제 y를 구할수있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch 모델의 기본 구조\n",
    "\n",
    "1. PyTorch 내장 모델 뿐만 아니라 사용자 정의 모델도 반드시 이 구조를 따라야 한다.\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model_Name(nn.Module):\n",
    "    def __init__(self):\n",
    "    \n",
    "        super(Model_Name, self).__init__()\n",
    "        self.module1 = ...\n",
    "        self.module2 = ...\n",
    "        \n",
    "        \"\"\"\n",
    "        ex)\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5)\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = some_function1(x)\n",
    "        x = some_function2(x)\n",
    "        \n",
    "        \"\"\"\n",
    "        ex)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \"\"\"\n",
    "        return x\n",
    "        \n",
    "model = Model_Name() # 여기에 변수를 넣어주면 됨.\n",
    "\n",
    "2. nn.Module\n",
    "\n",
    "__init__(self): initialize; 내가 사용하고 싶은, 내 신경망 모델에 사용될 구성품들을 정의 및 초기화 하는 메소드이다. \n",
    "forward(self, x): specify the connections;  이닛에서 정의된 구성품들을 연결하는 메소드이다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 틀린코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2): # init함수에서는 내가 나중에 forward에서 쓸 함수를 미리 선언하는것\n",
    "        self.input_dim = input_dim #input dimension = n\n",
    "        self.output_dim = output_dim # output dimension이 = m\n",
    "        \n",
    "        super().__init__()\n",
    "        # W와 b를 생성한다.\n",
    "        self.W = torch.FloatTensor(input_dim, output_dim) #([3,2])로 생성\n",
    "        self.b = torch.FloatTensor(output_dim) #([2])라는 차원의 벡터로 생성\n",
    "\n",
    "    # You should override 'forward' method to implement detail.\n",
    "    # The input arguments and outputs can be designed as you wish.\n",
    "    \n",
    "    def forward(self, x): #forward는 행동을 선언한다\n",
    "        # |x| = (batch_size(병렬로 처리할 샘플의 숫자), input_dim(입력벡터의 크기))\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "        #     = (batch_size, output_dim)\n",
    "        \n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear\n",
    "\n",
    "* torch.nn.Linear(input_size, output_size)\n",
    "* 들어오는 float32형 input 데이터에 대해 y=wx+b 형태의 선형 변환을 수행하는 메소드인 것을 알 수 있다.\n",
    "* Linear를 취할 때 정수형은 float으로 만들어 소수점 .0을 달아줘야한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "\n",
    "y = linear(x) # x통과시키면 y가 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 코드들 수행안됨. 잘못된 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is no weight parameters to learn.\n",
    "Above way can forward(or calculate) values, but it cannot be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 올바른 코드 Correct way: nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim)) #nn.Parameter 를 써야함\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "        #     = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter\n",
    "\n",
    "A kind of Tensor that is to be considered a module parameter.\n",
    "\n",
    "Parameters are Tensor subclasses, that have a very special property when used with Module s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000e+00, -0.0000e+00],\n",
      "        [ 9.1002e+31, -2.8586e-42],\n",
      "        [ 8.4078e-45,  0.0000e+00]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([4.7428e+30, 7.1429e+31], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(3, 2)\n",
    "\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters = weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3386, -0.5355, -0.4991],\n",
      "        [ 0.1993,  0.4776,  0.1894]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1819, 0.0941], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters(): # parameters() 메소드는 모듈의 파라메터들을 iterator로 반환함\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module can contain other nn.Module's child classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = self.linear(x)\n",
    "        # |y| = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1267,  0.0563,  0.3951],\n",
      "        [ 0.2291,  0.3214,  0.2595]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3659, 0.4013], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
